# Explanatory file.
# Usage:
#   python main.py train planck_train.yaml -v

# Output folder (str)
output: output/planck/train


# Emulator architecture.
emulator:
  # Name of emulator (Options: ffnn_emu)
  name: ffnn_emu
  # Parameters typical of the ffnn_model.
  # These are the ones used to build the architecture
  args:
    # Any activation function from (str)
    # https://keras.io/api/layers/activations/
    activation: sigmoid
    # Number of neurons for each hidden layer (list of positive int)
    neurons_hidden: [1000, 1000]
    # Normalize tensors with mean and variance (bool)
    batch_normalization: False
    # Relative dropout during training (float). It helps with overfitting.
    dropout_rate: 0.5
    # Any optimizer from (str)
    # https://www.tensorflow.org/api_docs/python/tf/keras/optimizers
    optimizer: adam
    # Name of the loss function (str). Options: any of the functions
    # defined in https://keras.io/api/losses/ (prepending keras.losses.),
    # plus the ones defined in src/emu_like/loss_functions.py
    loss: mean_squared_error
    # Number of epochs to be run before stop (int).
    # To keep track of resuming, this is converted and saved as a list.
    # In case a list is provided, the last value is the total number of
    # epochs to be run.
    epochs: 1000
    # Divide sample into batches of this size (int). The pipeline is
    # calculated individually in each of these batches and then
    # the loss function takes care of combining each batch.
    batch_size: 256
    # If False remove the output layer (bool, default: True).
    # Useful to reduce to linear regression case.
    want_output_layer: True
    # Learning rate (float). It is the equivalent of
    # the stepsize in gradient descent. The smaller it is the slower
    # will be to learn, but it will be more accurate to find minimum.
    # To keep track of resuming, this is converted and saved as a list.
    # In case a list is provided, the last value is the one used.
    learning_rate: 1.e-3


# Load an existing dataset. It can be generated by this same code running:
# python main.py sample spectra_sample.yaml -v
datasets:
  # Paths to the dataset (list of str).
  # It is possible to use multiple datasets for a single training,
  # just add them to the list of paths.
  # Options:
  # - list of folders: use the argument "paths". This is designed to work
  #   if the datasets are generated with this code. Each path is expected
  #   to contain a setting file, as well as x and y samples.
  # - list of files: if a single file contains both x and y use "paths".
  #   If x and y are stored in separate files use "paths_x" and "paths_y".
  paths:
    - output/planck/sample
  # In alternative, one can use different paths for x and y (list of str)
  paths_x:
  paths_y:

  # List of columns for x and y (list of int). Defaults:
  # If two different files paths are specified:
  #   - columns_x: [all columns]
  #   - columns_y: [all columns]
  # If a single file is specified:
  #   - columns_x: [all columns except last]
  #   - columns_y: [last column]
  columns_x:
  columns_y: [4]

  # Since in a single dataset it is possible to have different y files,
  # here it is possible to specify the name of the single dataset we
  # want to use for training.
  name:

  # Remove data with non finite y (inf and nan)
  remove_non_finite: False

  # Fractional samples used for training (float)
  frac_train: 0.9
  # Random seed for train/test splitting (int)
  train_test_random_seed: 1543
  # Rescale x and y variables. Options:
  # - None: does not perform rescaling
  # - StandardScaler: gaussian with mean 0 and standard deviation 1
  # - MinMaxScaler: min and max rescaled in the range [0, 1]
  # - MinMaxScalerPlus1: min and max rescaled in the range [1, 2]
  # - ExpMinMaxScaler: apply exponential and then MinMaxScaler
  rescale_x: MinMaxScaler
  rescale_y: ExpMinMaxScaler
