# Explanatory file.
# Usage:
#   python emu_like.py train planck_train.yaml -v

# Output folder (str)
output: output/planck/train


# Emulator architecture.
emulator:
  # Type of emulator (Options: ffnn_emu)
  type: ffnn_emu
  # Parameters typical of the ffnn_model.
  # These are the ones used to build the architecture
  params:
    # Any activation function from (str)
    # https://keras.io/api/layers/activations/
    activation: sigmoid
    # Number of neurons for each hidden layer (list of positive int)
    neurons_hidden: [1000, 1000]
    # Normalize tensors with mean and variance (bool)
    batch_normalization: False
    # Relative dropout during training (float). It helps with overfitting.
    dropout_rate: 0.5
    # Any optimizer from (str)
    # https://www.tensorflow.org/api_docs/python/tf/keras/optimizers
    optimizer: adam
    # Name of the loss function (str). Options: any of the functions
    # defined in https://keras.io/api/losses/ (prepending keras.losses.),
    # plus the ones defined in src/emu_like/loss_functions.py
    loss: keras.losses.mean_squared_error
    # Number of epochs to be run before stop (int).
    # To keep track of resuming, this is converted and saved as a list.
    # In case a list is provided, the last value is the total number of
    # epochs to be run.
    epochs: 1000
    # Divide sample into batches of this size (int). The pipeline is
    # calculated individually in each of these batches and then
    # the loss function takes care of combining each batch.
    batch_size: 256
    # If False remove the output layer (bool, default: True).
    # Useful to reduce to linear regression case.
    want_output_layer: True
    # Learning rate (float). It is the equivalent of
    # the stepsize in gradient descent. The smaller it is the slower
    # will be to learn, but it will be more accurate to find minimum.
    # To keep track of resuming, this is converted and saved as a list.
    # In case a list is provided, the last value is the one used.
    learning_rate: 1.e-3


# Load an existing sample. It can be generated by this same code running:
# python emu_like.py sample planck_sample.yaml -v -p
training_sample:
  # Paths to the sample (list of str).
  # It can be either a list of folders created with this code (the structure
  # is known and some additional info can be inferred, e.g. the name of
  # parameters), or they can point to files.
  paths:
    - output/planck/sample
  # # In alternative, one can use different paths for x and y (list of str)
  # paths_x:
  # paths_y:
  # # List of columns x and y variables. Defaults:
  # # If a single path is specified:
  # #   - columns_x: all except last
  # #   - columns_y: last one
  # # If two different paths are specified:
  # #   - columns_x: all
  # #   - columns_y: all
  # columns_x: [1, 2]
  columns_y: [4]
  # Remove data with non finite y (inf and nan)
  remove_non_finite: False

  # Fractional samples used for training (float)
  frac_train: 0.9
  # Random seed for train/test splitting (int)
  train_test_random_seed: 1543
  # Rescale x and y variables. Options:
  # - None: does not perform rescaling
  # - StandardScaler: gaussian with mean 0 and standard deviation 1
  # - MinMaxScaler: min and max rescaled in the range [0, 1]
  # - MinMaxScalerPlus1: min and max rescaled in the range [1, 2]
  # - ExpMinMaxScaler: apply exponential and then MinMaxScaler
  rescale_x: MinMaxScaler
  rescale_y: ExpMinMaxScaler
