# Explanatory file.

# Output folder (str)
output: output/test_model

# Type of emulator (Options: ffnn_emu)
emulator_type: ffnn_emu


# Load an existing sample. It can be generated by this same code running:
# python emu_like.py sample explanatory_sample.yaml -v -p
training_sample:
  # Path to the sample (str).
  # It can be either a folder created with this code (the structure is
  # known and some additional info can be inferred, e.g. the name of
  # parameters), or it can point to a file.
  path: output/test_sample
  # # In alternative, one can use two different paths for x and y (str)
  # path_x:
  # path_y:
  # # List of columns x and y variables. Defaults:
  # # If a single path is specified:
  # #   - columns_x: all except last
  # #   - columns_y: last one
  # # If two different paths are specified:
  # #   - columns_x: all
  # #   - columns_y: all
  # columns_x: [1, 2]
  # columns_y: [-1]

# Fractional samples used for training (float)
frac_train: 0.9
# Random seed for train/test splitting (int)
train_test_random_seed: 1543
# Rescale x and y variables. Options:
# - None: does not perform rescaling
# - StandardScaler: gaussian with mean 0 and standard deviation 1
# - MinMaxScaler: min and max rescaled in the range [0, 1]
# - MinMaxScalerPlus1: min and max rescaled in the range [1, 2]
rescale_x: StandardScaler
rescale_y: MinMaxScalerPlus1

# Parameters typical of the ffnn_model.
# These are the ones used to build the architecture
ffnn_model:
  # Any activation function from (str)
  # https://keras.io/api/layers/activations/
  activation_function: relu
  # Number of neurons for each hidden layer (list of positive int)
  neurons_hidden_layer: []
  # Normalize tensors with mean and variance (bool)
  batch_normalization: False
  # Relative dropout during training (float). It help with overfitting.
  dropout_rate: 0.
  # Any optimizer from (str)
  # https://www.tensorflow.org/api_docs/python/tf/keras/optimizers
  optimizer: adam
  # Name of the loss function (str). Options: any of the functions
  # defined in https://keras.io/api/losses/, plus the ones
  # defined in tools/loss_functions.py
  loss_function: mean_squared_error
  # Number of epochs to be run before stop (int)
  n_epochs: 1000
  # Divide sample into batches of this size (int). The pipeline is
  # calculated individually in each of these batches and then
  # the loss function takes care of combining each batch.
  batch_size: 32
  # If False remove the output layer (default: True).
  # Useful to reduce to linear regression case.
  want_output_layer: True
  # Learning rate (float). It is the equivalent of
  # the stepsize in gradient descent. The smaller it is the slower
  # will be to learn, but it will be more accurate to find minimum.
  learning_rate: 1.e-3
