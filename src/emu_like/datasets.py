"""
.. module:: datasets

:Synopsis: Deal with datasets.
:Author: Emilio Bellini

"""

import numpy as np
import os
import re
import sklearn.model_selection as skl_ms
import tqdm
from . import io as io
from . import scalers as sc
from . import pca
from .x_samplers import XSampler
from .y_models import YModel


class Dataset(object):
    """
    This class is primarly meant to store a dataset that can be used to
    train/test a ML algorithm. Here it is assumed that both x and y are
    2D arrays with n_samples rows and n_x, n_y columns.
    To load a dataset there are two ways:
    - create a Dataset instance and manually define all the required
      attributes;
    - create a DataCollection instance and load the dataset. Use the
      DataCollection.get_one_y_dataset(name) method to extract the
      desired Dataset.
    This is more flexible than DataCollection, since it allows to load
    non standard datasets (use .load_external). In this case, the code
    has two options:
    - x and y from two different files.
    - x and y from a single file. In this case the default behaviour is
      that the last column is y, and the remaining are all x's.
    Main methods:
    - load: load a dataset generated by this code;
    - load_external: load a non-standard dataset;
    - join: if they are compatible, join two datasets and return a
      single one;
    - train_test_split: split a dataset into train and test samples;
    - rescale: rescale dataset (both x and y);
    - apply_pca: apply PCA (both x and y).

    NOTE: For all the dataset generation, load, save operations, use the
    DataCollection class.
    """

    def __init__(
            self,
            name=None,
            x=None,
            y=None,
            n_x=None,
            n_y=None,
            n_samples=None,
            x_names=None,
            y_names=None,
            y_model=None,
            x_ranges=None,
            x_scaler=None,
            y_scaler=None,
            x_pca=None,
            y_pca=None,
            path=None
            ):
        """
        Placeholders.
        """
        # Name
        self.name = name
        # Data arrays
        self.x = x  # x
        self.y = y  # y
        # Data arrays - Train/test split
        self.x_train = None  # x_train
        self.y_train = None  # y_train
        self.x_test = None  # x_test
        self.y_test = None  # y_test

        # Data shapes
        self.n_x = n_x  # Number of x variables
        self.n_y = n_y  # Number of y variables
        self.n_samples = n_samples  # Number of samples

        # Labels
        self.x_names = x_names  # List of names of x data
        self.y_names = y_names  # List of names of y data
        self.x_key = None # Name of the x image in fits file

        # y_model
        self.y_model = y_model

        # Path
        self.path = path

        # Container for all the settings
        self.settings = None

        # Placeholders
        self.x_scaler = x_scaler
        self.y_scaler = y_scaler
        self.x_pca = x_pca
        self.y_pca = y_pca

        self.x_ranges = x_ranges

        return

    @staticmethod
    def _load_array(path):
        """
        Load an array from file.
        Arguments:
        - path (str): path to the array
        - columns (default: None): slice object or list of
          column indices to be read.
        """
        array = np.genfromtxt(path)
        # Adjust array dimensions.
        # If it has one feature I still want 1x2_samples
        if array.ndim == 1:
            array = array[:, np.newaxis]
        return array

    @staticmethod
    def _try_to_load_names_array(
            path,
            n_names=None,
            comments='#',
            delimiter='\t'):
        """
        Try to load name of parameters from array.
        Names are extracted from the last comment row
        (starting with 'comments') of the file and should
        match the number of columns of the array.
        Arguments:
        - path (str): path to the array;
        - n_names (int, default: None): number of names to
          be expected. It is used to validate the extracted names.
          If they do not match, this method returns None;
        - comments (str, default: '#'). Starting string for comments;
        - delimiter (str, default: '\t'). Separator for column names.

        """
        is_comment = True
        names = None
        # Look for the last line that starts with 'comments'
        with open(path, 'r') as fn:
            while is_comment:
                line = fn.readline()
                if line.startswith(comments):
                    names = line
                else:
                    is_comment = False
        # Split names
        try:
            names = re.sub(comments, '', names)
        except TypeError:
            return None
        names = names.split(delimiter)
        names = [x.strip() for x in names]
        # Check names have the right dimensions
        if n_names:
            if n_names == len(names):
                return names
            else:
                return None
        return names

    def slice(self, columns_x, columns_y, verbose=False):
        """
        Given a Dataset select the columns wanted, both for
        "x" and "y". It adjusts also the other attributes.
        Arguments:
        - columns_x (list of indices or slice object). Default: if "x"
          and "y" data come from different files all columns. If "x" and
          "y" are in the same file, all columns except the last one;
        - columns_y (list of indices or slice object for each y file).
          Default: if "x" and "y" data come from different files all
          columns. If "x" and "y" are in the same file, last column;
        - verbose (bool, default: False): verbosity.
        """

        if verbose:
            io.print_level(
                1, 'Slicing x data with columns: {}.'.format(columns_x))
            io.print_level(
                1, 'Slicing y data with columns: {}.'.format(columns_y))

        def slice_list(lst, slicing):
            if isinstance(slicing, list):
                return [lst[i] for i in slicing]
            elif isinstance(slicing, slice):
                return lst[slicing]
            else:
                raise Exception('Check your slicing, it can be either a list '
                                'or a slice object!')

        # Change default columns
        if columns_x is None:
            columns_x = slice(None)
        if columns_y is None:
            columns_y = slice(None)

        # Data
        self.x = self.x[:, columns_x]
        self.y = self.y[:, columns_y]

        # Names
        self.x_names = slice_list(self.x_names, columns_x)
        self.y_names = slice_list(self.y_names, columns_y)

        # Adjust shapes
        self.n_samples, self.n_x = self.x.shape
        _, self.n_y = self.y.shape

        return self

    def remove_non_finite(self, store_non_finites=False, verbose=False):
        """
        Remove from the dataset non finite samples (inf and nan).
        Arguments:
        - store_non_finites (bool, default: False): store x's that
          give non finite y in non_finites_x.
        - verbose (bool, default: False): verbosity.
        """

        if verbose:
            io.print_level(1, 'Removing non finite values from x and y.')

        # Finite indices
        only_finites = np.all(np.isfinite(self.y), axis=1)

        # Sore non finite elements
        if store_non_finites:
            only_non_finites = np.array([not elem for elem in only_finites])
            self.non_finites_x = self.x[only_non_finites]

        self.x = self.x[only_finites]
        self.y = self.y[only_finites]

        # Adjust n_samples
        self.n_samples = self.x.shape[0]

        return self

    def load(
            self,
            path,
            name=None,
            columns_x=None,
            columns_y=None,
            verbose=False):
        """
        Load an existing dataset.
        This method assumes that path points to a dataset generated by
        this code (see NOTE below).
        Arguments:
        - path (str): path pointing to the fits file containing the dataset;
        - name (str): name of the dataset if there are many stored;
        - columns_x (list of indices or slice object). Default: if "x"
          and "y" data come from different files all columns. If "x" and
          "y" are in the same file, all columns except the last one;
        - columns_y (list of indices or slice object for each y file).
          Default: if "x" and "y" data come from different files all
          columns. If "x" and "y" are in the same file, last column;
        - verbose (bool, default: False): verbosity.

        NOTE: when generated by this code, the dataset file has a specific
        format (see discussion at the top of this Class). If this is not
        the case use the Dataset.load_external() method, which will allow
        to perform all training/testing operations.
        """

        if verbose:
            io.info('Loading dataset.')

        # Init fits file
        fits = io.FitsFile(path)

        # Load settings
        self.settings = fits.get_header(0, unflat_dict=True)

        # Main path
        self.path = path
        # Store y name
        self.name = name

        if columns_x is None:
            columns_x = slice(None)
        if columns_y is None:
            columns_y = slice(None)

        # Init x sampler
        x_sampler = XSampler.choose_one(
            self.settings['x_sampler']['name'],
            self.settings['params'],
            **self.settings['x_sampler']['args'],
            verbose=False)

        # Load x data.
        x_sampler.x = fits.get_data(x_sampler.x_key)
        self.x = x_sampler.x

        # Get remaining x attributes
        self.n_x = x_sampler.get_n_x()
        self.n_samples = x_sampler.get_n_samples()
        self.x_names = x_sampler.get_x_names()
        self.x_key = x_sampler.x_key

        if self.settings is None:
            self.x_ranges = [(m, M) for m, M in zip(self.x.min(axis=0), self.x.max(axis=0))]
        else:
            self.x_ranges = [(
                self.settings['params'][name]['prior']['min'],
                self.settings['params'][name]['prior']['max']) for name in self.x_names]

        # Init y_model
        y_model = YModel.choose_one(
            self.settings['y_model']['name'],
            self.settings['params'],
            {name: self.settings['y_model']['outputs'][name]},
            self.n_samples,
            **self.settings['y_model']['args'],
            verbose=False)
        
        # Load y_model
        y_model.load(
            self.path,
            verbose=False,
        )

        # Load y data.
        y_model.y = fits.get_data(name)
        self.y = y_model.y

        # Get remaining y attributes
        self.n_y = y_model.get_n_y()
        self.y_names = y_model.get_y_names()
        self.y_headers = y_model.get_y_headers()

        # Propagate x_sampler and y_model
        self.x_sampler = x_sampler
        self.y_model = y_model

        # Print info
        if verbose:
            io.print_level(1, 'Loaded dataset from: {}'.format(self.path))

        return self

    def load_external(
            self,
            path,
            path_y=None,
            columns_x=None,
            columns_y=None,
            verbose=False):
        """
        Load an existing dataset.
        This method if more flexible than the one defined in
        Dataset.load(), because it allows to load non-standard
        datasets, but it loads the minimum amount of information
        needed to train an ML algorithm.
        It accepts only files in two formats:
        - a single file containing both x and y data (use "path");
        - two files, one for x data ("path") and one for "y" (path_y).
        Arguments:
        - path (str): path pointing to the file containing the "x" data.
          If it contains both "x" and "y", "path_y" should be None;
        - path_y (str, default: None): in case "x" and "y" data are stored in
          different files, use this variable to specify the file containing
          the "y" data;
        - columns_x (list of indices or slice object). Default: if "x"
          and "y" data come from different files all columns. If "x" and
          "y" are in the same file, all columns except the last one;
        - columns_y (list of indices or slice object for each y file).
          Default: if "x" and "y" data come from different files all
          columns. If "x" and "y" are in the same file, last column;
        - verbose (bool, default: False): verbosity.
        """

        if verbose:
            io.info('Loading dataset.')

        # Define paths. Cases:
        # 1) One file for both x and y
        if os.path.isfile(path) and path_y is None:
            path_x = path
            path_y = path
            self.path = path
            # Change default columns
            if columns_x is None:
                columns_x = slice(None, -1)
            if columns_y is None:
                columns_y = slice(-1, None)
        # 2) One file for x and one for y
        elif os.path.isfile(path) and os.path.isfile(path_y):
            path_x = path
            path_y = path_y
            self.path = [path, path_y]
            # Change default columns
            if columns_x is None:
                columns_x = slice(None)
            if columns_y is None:
                columns_y = slice(None)
        else:
            raise Exception('Something is wrong with your paths. '
                            'Dataset could not be loaded!')

        # Load data
        self.x  = self._load_array(path_x)
        self.y = self._load_array(path_y)

        # Get shapes
        self.n_samples, self.n_x = self.x.shape
        _, self.n_y = self.y.shape

        # Try to infer the names
        self.x_names = Dataset._try_to_load_names_array(path_x, n_names=self.n_y)
        self.y_names = Dataset._try_to_load_names_array(path_y, n_names=self.n_y)

        # Slice data
        self.slice(columns_x, columns_y, verbose=verbose)

        # Print info
        if verbose:
            io.print_level(1, 'x from: {}'.format(path_x))
            io.print_level(1, 'y from: {}'.format(path_y))
            io.print_level(1, 'n_samples: {}'.format(self.n_samples))
            io.print_level(1, 'n_x: {}'.format(self.n_x))
            io.print_level(1, 'n_y: {}'.format(self.n_y))

        return self

    @staticmethod
    def join(datasets, verbose=False):
        """
        Join a list of datasets into a unique one.
        This defines the minimum number of attributes
        required to use a dataset for tranining, i.e.
        x, y, n_x, n_y, n_samples, x_names and y_names.
        Before joining them it checks that n_x and n_y are
        the same for each dataset.
        Arguments:
        - datasets (list of Dataset): list of Dataset classes (already loaded);
        - verbose (bool, default: False): verbosity.
        """

        if verbose:
            io.info('Joining datasets')
            for dataset in datasets:
                io.print_level(1, '{}'.format(dataset.path))

        data = Dataset()

        # Name
        data.name = datasets[0]

        # n_x
        if all(s.n_x == datasets[0].n_x for s in datasets):
            data.n_x = datasets[0].n_x
        else:
            raise ValueError('Datasets can not be joined as they have '
                             'different number of x variables')
        
        # n_y
        if all(s.n_y == datasets[0].n_y for s in datasets):
            data.n_y = datasets[0].n_y
        else:
            raise ValueError('Datasets can not be joined as they have '
                             'different number of x variables')

        # x array
        total = tuple([s.x for s in datasets])
        data.x = np.vstack(total)

        # y array
        total = tuple([s.y for s in datasets])
        data.y = np.vstack(total)

        # x and y names
        data.x_names = datasets[0].x_names
        data.y_names = datasets[0].y_names
        data.x_key = datasets[0].x_key

        # n_samples
        data.n_samples = sum([s.n_samples for s in datasets])

        # y_model.
        # NOTE: we are assuming that all datsets are sharing the
        # same y_model, which is taken from the first one.
        data.y_model = datasets[0].y_model

        # Adjust YModel params
        for var in datasets[0].y_model.params:
            mins = [dat.y_model.params[var]['prior']['min'] for dat in datasets]
            maxs = [dat.y_model.params[var]['prior']['max'] for dat in datasets]
            data.y_model.params[var]['prior']['min'] = min(mins)
            data.y_model.params[var]['prior']['max'] = max(maxs)

        return data

    def train_test_split(self, frac_train, seed, verbose=False):
        """
        Split a dataset into test and train samples.
        The split is stored into the x_train, x_test,
        y_train and y_test attributes.
        Arguments:
        - frac_train (float): fraction of training samples (between 0 and 1);
        - seed (int): seed to randomly split train and test;
        - verbose (bool, default: False): verbosity.

        NOTE: this method assumes that both settings and the fulle x array
        are already saved into the folder. The x array is then used to
        calculate the missing row of the y array.
        """

        if verbose:
            io.info('Splitting dataset in training and testing samples.')
            io.print_level(1, 'Fractional number of training samples: {}'
                           ''.format(frac_train))
            io.print_level(1, 'Random seed for train/test split: '
                           '{}'.format(seed))
        split = skl_ms.train_test_split(self.x, self.y,
                                        train_size=frac_train,
                                        random_state=seed)
        self.x_train, self.x_test, self.y_train, self.y_test = split
        return

    def rescale(self, rescale_x, rescale_y, verbose=False):
        """
        Rescale x and y of a dataset. The available scalers are
        written in src/emu_like/scalers.py
        Arguments:
        - rescale_x (str): scaler for x;
        - rescale_y (str): scaler for y;
        - verbose (bool, default: False): verbosity.

        NOTE: this method assumes that we already splitted
        train and test samples.
        """

        if verbose:
            io.info('Rescaling x and y.')
            io.print_level(1, 'x with: {}'.format(rescale_x))
            io.print_level(1, 'y with: {}'.format(rescale_y))
        # Rescale x
        self.x_scaler = sc.Scaler.choose_one(rescale_x)
        self.x_scaler.fit(self.x_train)
        self.x_train = self.x_scaler.transform(self.x_train)
        self.x_test = self.x_scaler.transform(self.x_test)
        # Rescale y
        self.y_scaler = sc.Scaler.choose_one(rescale_y)
        self.y_scaler.fit(self.y_train)
        self.y_train = self.y_scaler.transform(self.y_train)
        self.y_test = self.y_scaler.transform(self.y_test)
        if verbose:
            io.print_level(1, 'Rescaled bounds:')
            mins = np.min(self.x_train, axis=0)
            maxs = np.max(self.x_train, axis=0)
            for nx, min in enumerate(mins):
                io.print_level(
                    2, 'x_train_{} = [{}, {}]'.format(nx, min, maxs[nx]))
            mins = np.min(self.x_test, axis=0)
            maxs = np.max(self.x_test, axis=0)
            for nx, min in enumerate(mins):
                io.print_level(
                    2, 'x_test_{} = [{}, {}]'.format(nx, min, maxs[nx]))
            mins = np.min(self.y_train, axis=0)
            maxs = np.max(self.y_train, axis=0)
            for nx, min in enumerate(mins):
                io.print_level(
                    2, 'y_train_{} = [{}, {}]'.format(nx, min, maxs[nx]))
            mins = np.min(self.y_test, axis=0)
            maxs = np.max(self.y_test, axis=0)
            for nx, min in enumerate(mins):
                io.print_level(
                    2, 'y_test_{} = [{}, {}]'.format(nx, min, maxs[nx]))
        return

    def apply_pca(self, num_x_pca=None, num_y_pca=None, verbose=False):
        """
        Apply PCA to x and/or y of a dataset.
        Arguments:
        - num_pca_x (int): number of modes to be retained
          for x (if 0 or negative PCA is not applied);
        - num_pca_y (int): number of modes to be retained
          for y (if 0 or negative PCA is not applied);
        - verbose (bool, default: False): verbosity.

        NOTE: this method assumes that we already splitted
        train and test samples.
        """

        if num_x_pca == 'None':
            num_x_pca = None
        if num_y_pca == 'None':
            num_y_pca = None

        if verbose:
            if num_x_pca is not None:
                io.info('Applying PCA on x. Number of modes retained {}.'.format(num_x_pca))
            if num_y_pca is not None:
                io.info('Applying PCA on y. Number of modes retained {}.'.format(num_y_pca))

        # PCA x
        self.x_pca = pca.PCA(n_components=num_x_pca)
        self.x_pca.fit(self.x_train)
        self.x_train = self.x_pca.transform(self.x_train)
        self.x_test = self.x_pca.transform(self.x_test)
        # PCA y
        self.y_pca = pca.PCA(n_components=num_y_pca)
        self.y_pca.fit(self.y_train)
        self.y_train = self.y_pca.transform(self.y_train)
        self.y_test = self.y_pca.transform(self.y_test)

        return


class DataCollection(object):
    """
    This class that is primarly meant to deal with the generation
    of datasets. All the "y" attributes are stored in lists.
    Therefore, it is possible to manage multiple "y" outputs from
    the same YModel (e.g., see YModel.ClassSpectra).
    In the simplest cases, where there is just one "y", everything
    is stored in single element lists.
    If you want to use a dataset for training/testing a ML algorithm
    use the Dataset class. If you have a DataCollection, it is
    possible to get a Dataset in three ways:
    - create a Dataset instance and manually define all the required
      attributes;
    - if there is just one "y", it is possible to use the
      DataCollection.get_one_y_dataset() method to get the single
      dataset;
    - if there are multiple "y", select the data you want to
      extract using the DataCollection.get_one_y_dataset(name) method.

    Available methods:
    - load: load a data collection from a file;
    - save: save a data collection to a file;
    - sample: generate a dataset;
    - resume: after loading a dataset, compute the remaining samples;

    NOTE: the data collections generated by this code contain one fits
    file, that has its settings stored as header in the PrimaryHDU,
    while x and y points are stored with the respective keywords in
    other images. Depending on the YModel, there are stored other
    images necessary for that model.
    For flexibility, i.e. when they were created from other codes,
    use the Dataset class.
    """

    def __init__(self):
        """
        Placeholders.
        """
        # Data arrays
        self.x = None  # x
        self.y = []  # y per file

        # Data shapes
        self.n_x = None  # Number of x variables
        self.n_y = []  # Number of y variables per file
        self.n_samples = None  # Number of samples

        # Labels
        self.x_names = None  # List of names of x data
        self.y_names = []  # List of names of y data per file
        self.y_headers = []  # Headers for y files
        self.x_key = None # Name of the x image in fits file
        self.y_keys = [] # Name of the y images in fits file

        # Paths
        self.path = None  # Path of the dataset

        # Container for all the settings
        self.settings = None

        # Placeholder for the YModel
        self.y_model = None

        # Useful to keep track of how many samples have been computed
        self.counter_samples = 0

        return

    def get_one_y_dataset(self, name=None):
        """
        Extract one datase from DataCollection.
        This dataset can be specified by its name, or it defaults to
        the only dataset if there is just one. If there are multiple
        datasets and no name is specified it raises an Exception.
        Arguments:
        - name (str, default:None): if specified it gets the y dataset
          with that name.
        """
        # Get correct index
        if name is not None:
            idx = self.y_model.spectra.names.index(name)
        elif len(self.y_model.spectra.names) == 1:
            idx = 0
        else:
            raise Exception(
                'It is not possible to extract a single dataset if no name '
                'is specified and there are multiple datasets!')

        dataset = Dataset(
            name=name,
            x=self.x,
            y=self.y[idx],
            n_x=self.n_x,
            n_y=self.n_y[idx],
            n_samples=self.n_samples,
            x_names=self.x_names,
            y_names=self.y_names[idx],
            y_model=self.y_model[idx],
            path=self.path,
        )

        return dataset

    def save(
            self,
            fname=None,
            root=None,
            settings=None,
            data_x=None,
            data_ys=None,
            hd_x=None,
            hd_ys=None,
            name_x=None,
            name_ys=None,
            y_model=None,
            verbose=False
            ):
        """
        Save dataset to path.
        Arguments:
        - fname (str, default: None): file name;
        - root (str, default: None): eventually root;
        - settings (dict, default: None): settings dictionary;
        - data_x (array, default: None): 2D array;
        - data_ys (list, default: None): list of 2D arrays;
        - hd_x (str or dict, default: None): string or dict for x header;
        - hd_ys (list, default: None): list of strings or dicts for x header;
        - name_x (str, default: None): keyword for data_x;
        - name_ys (str, default: None): list of keywords for data_ys;
        - y_model (emu_like.YModel, default None): model to save;
        - verbose (bool, default: False): verbosity.
        """

        fits = io.FitsFile(
            fname=fname,
            root=root,
        )

        # Save settings
        if settings is None:
            settings = self.settings
        fits.write(
            data=None,
            header=self.settings,
            name=None,
            verbose=verbose,
        )

        # Save x
        if data_x is None:
            data_x = self.x
        if name_x is None:
            name_x = self.x_key
        fits.write(
            data=data_x,
            header=hd_x,
            name=name_x,
            verbose=verbose,
        )

        # Save y
        if data_ys is None:
            data_ys = self.y
        if name_ys is None:
            name_ys = self.y_keys
        if hd_ys is None:
            hd_ys = [None for _ in range(len(data_ys))]
        for idx in range(len(data_ys)):
            fits.write(
                data=data_ys[idx],
                header=hd_ys[idx],
                name=name_ys[idx],
                verbose=verbose,
            )
        
        # Save y_model
        if y_model is None:
            y_model = self.y_model
        y_model.save(
            fname=self.path,
            verbose=verbose)

        return

    def load(
            self,
            path,
            verbose=False):
        """
        Load an existing data collection.
        This method assumes that path points to a dataset generated by
        this code (see NOTE below).
        Arguments:
        - path (str): path pointing to the file containing the data
          collection. For non standard samples use Dataset.load_external;
        - verbose (bool, default: False): verbosity.

        NOTE: when generated by this code, the dataset file has a specific
        format (see discussion at the top of this Class). If this is not
        the case use the Dataset.load_external() method, which will allow
        to perform all training/testing operations.
        """

        if verbose:
            io.info('Loading data collection.')
        
        # Init fits file
        fits = io.FitsFile(path)

        # Load settings
        self.settings = fits.get_header(0, unflat_dict=True)

        # Main path
        self.path = path

        # Init x sampler
        x_sampler = XSampler.choose_one(
            self.settings['x_sampler']['name'],
            self.settings['params'],
            **self.settings['x_sampler']['args'],
            verbose=False)

        # Load x data.
        x_sampler.x = fits.get_data(x_sampler.x_key)
        self.x = x_sampler.x

        # Get remaining x attributes
        self.n_x = x_sampler.get_n_x()
        self.n_samples = x_sampler.get_n_samples()
        self.x_names = x_sampler.get_x_names()
        self.x_key = x_sampler.x_key

        # Init y_model
        y_model = YModel.choose_one(
            self.settings['y_model']['name'],
            self.settings['params'],
            self.settings['y_model']['outputs'],
            self.n_samples,
            **self.settings['y_model']['args'],
            verbose=False)
        
        # Load y_model
        y_model.load(
            self.path,
            verbose=False,
        )

        # Load y data.
        self.y_keys = y_model.spectra.names
        # 1) load ys.
        y = [fits.get_data(name) for name in self.y_keys]
        # 2) Infer dimensions
        n_rows = [y_one.shape[0] for y_one in y]
        n_y = [y_one.shape[1] for y_one in y]

        # Check and init counter_samples
        if not all(row == n_rows[0] for row in n_rows):
            raise IOError('Not all the files have the same number of rows')
        self.counter_samples = n_rows[0]

        # 3) Initialize list of zeros arrays with full or remaining samples.
        y_model.y = [np.zeros((self.n_samples, n_y_one)) for n_y_one in n_y]

        # 4) Assign values.
        for ny_gen, y_gen in enumerate(y_model.y):
            y_gen[:self.counter_samples] = y[ny_gen]

        # 5) Synchronize with self.y.
        self.y = y_model.y

        # Get remaining y attributes
        self.n_y = y_model.get_n_y()
        self.y_names = y_model.get_y_names()
        self.y_headers = y_model.get_y_headers()

        # Propagate x_sampler and y_model
        self.x_sampler = x_sampler
        self.y_model = y_model

        # Print info
        if verbose:
            io.print_level(1, 'Loaded dataset from: {}'.format(self.path))

        return self

    def sample(
            self,
            params,
            x_name,
            y_name,
            x_args=None,
            y_args=None,
            y_outputs=None,
            output=None,
            verbose=False):
        """
        Generate a dataset.
        Arguments:
        - params (dict): dictionary containing the parameters to be passed
          to the y_model. See yaml files for details;
        - x_name (str): name of the x_sampler. Options defined in
          src/emu_like/x_samplers.py;
        - y_name (str): name of the y_model.
          Options defined in src/emu_like/y_models.py;
        - x_args (dict, default: None): dictionary with extra
          arguments needed by the x_sampler function;
        - y_args (dict, default: None): dictionary with extra
          arguments needed by the y_model;
        - y_outputs (dict, default: None): dictionary dealing
          with multiple y outputs for a single x (see class_spectra);
        - output (str, default: None): if None nothing is saved;
        - verbose (bool, default: False): verbosity.
        """

        # Preliminary checks on output
        save_it = False
        if output is not None:
            save_it = True
            self.path = output
            fits = io.FitsFile(self.path)
            if fits.exists:
                raise Exception(
                    'Output file exists! Exiting to avoid corruption of precious '
                    'data! If you want to resume a previous run use the '
                    '--resume (-r) option.')
            elif verbose:
                io.info('Generating dataset.')
                io.print_level(1, 'Writing output in {}'.format(output))

        # Create settings dictionary
        self.settings = {
            'x_sampler': {
                'name': x_name,
                'args': x_args,
            },
            'y_model': {
                'name': y_name,
                'args': y_args,
                'outputs': y_outputs,
            },
            'params': params,
        }
        # Save settings
        if save_it:
            fits.write(
                data=None,
                header=self.settings,
                name=None,
            )

        # Init x sampler
        x_sampler = XSampler.choose_one(
            x_name,
            params,
            **x_args,
            verbose=verbose)

        # Get x data and attributes
        self.x = x_sampler.get_x()
        self.n_x = x_sampler.get_n_x()
        self.n_samples = x_sampler.get_n_samples()
        self.x_names = x_sampler.get_x_names()
        self.x_key = x_sampler.x_key

        # Save x_array
        if save_it:
            fits.write(
                data=self.x,
                header=None,
                name=x_sampler.x_key,
            )

        # Init y_model
        y_model = YModel.choose_one(
            y_name,
            params,
            y_outputs,
            self.n_samples,
            **y_args,
            verbose=verbose)

        # Save after init what has to be saved
        if save_it:
            y_model.save(
                fname=self.path,
                verbose=verbose)

        # Get y attributes
        self.n_y = y_model.get_n_y()
        self.y_names = y_model.get_y_names()
        self.y_headers = y_model.get_y_headers()
        self.y_keys = y_model.spectra.names

        # Init self.y
        y_model.y = [np.zeros((self.n_samples, n_y)) for n_y in self.n_y]
        self.y = y_model.y

        # Start iteration in series
        for nx, x in enumerate(tqdm.tqdm(self.x)):
            y_one = y_model.evaluate(x, nx)
            self.counter_samples += 1

            if any([np.isnan(yy).any() for yy in y_one]):
                io.warning(' Found nans with parameters {}'.format(x))

            # Save array
            if save_it:
                for nname, name in enumerate(self.y_keys):
                    fits.append(
                        data=y_one[nname],
                        name=name,
                        header=self.y_headers[nname]
                    )

        # Propagate x_sampler and y_model
        self.x_sampler = x_sampler
        self.y_model = y_model

        return

    def resume(self, path, verbose=False):
        """
        Resume a dataset previously loaded (use load method
        before resuming). Many settings are already loaded.
        Arguments:
        - path (str): path pointing to the folder containing the dataset;
        - verbose (bool, default: False): verbosity.

        NOTE: this method assumes that both settings and the full x array
        are already saved into the folder. The x array is then used to
        calculate the missing row of the y array.
        """

        # Load the dataset
        self.load(path, verbose=verbose)

        if verbose:
            io.info('Resuming dataset computation.')
            io.print_level(1, 'From: {}'.format(self.path))
            io.print_level(
                1, 'Remaining samples: {}'
                ''.format(self.n_samples-self.counter_samples))
        if self.counter_samples == self.n_samples:
            if verbose:
                io.warning('Dataset complete, nothing to resume!')
            return

        fits = io.FitsFile(fname=path)
        start = self.counter_samples
        for ns, x in enumerate(tqdm.tqdm(self.x[start:])):
            y_one = self.y_model.evaluate(x, start + ns)
            self.counter_samples += 1
        
            # Save array
            for nname, name in enumerate(self.y_keys):
                fits.append(
                    data=y_one[nname],
                    name=name,
                )

        return

